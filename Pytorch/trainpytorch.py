import os
import glob
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset, random_split
from sklearn.model_selection import KFold
from sklearn.metrics import confusion_matrix, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import collections
from tqdm import tqdm 
# Importar las bibliotecas necesarias
import os  # Para realizar operaciones del sistema operativo
import glob  # Para encontrar nombres de archivos que coincidan con un patr칩n
import wfdb  # Para trabajar con registros y anotaciones de datos fisiol칩gicos
import sys  # Para manipular el int칠rprete de Python
import collections
import neurokit2 as nk  # Para procesar y analizar se침ales fisiol칩gicas
import numpy as np  # Para operaciones num칠ricas y trabajo con arrays
import seaborn as sns
import tensorflow as tf  # Para trabajar con modelos de aprendizaje profundo
from tensorflow import keras  # API de alto nivel de TensorFlow
from segment_signals import segmentSignals  # Funci칩n personalizada para segmentar se침ales
from sklearn.model_selection import train_test_split  # Para dividir datos en conjuntos de entrenamiento y prueba
from keras.models import load_model  # Para cargar modelos de aprendizaje profundo
from keras.utils import to_categorical  # Para convertir etiquetas a formato categ칩rico
from cnnpytorch import CNNModel  # Funci칩n personalizada para obtener un modelo CNN
from sklearn.model_selection import train_test_split, GridSearchCV, KFold  # Herramientas para validaci칩n cruzada y b칰squeda de hiperpar치metros
from scikeras.wrappers import KerasClassifier  # Envolver modelos Keras para usarlos con scikit-learn
from sklearn import metrics  # Para evaluar el rendimiento del modelo
import matplotlib.pyplot as plt  # Para generar gr치ficos
from sklearn.metrics import RocCurveDisplay, confusion_matrix, recall_score, f1_score  # Para mostrar curvas ROC
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())

# Par치metros constantes
FS = 500  # Frecuencia de muestreo
W_LEN = 256  # Longitud de la ventana para segmentar se침ales
W_LEN_1_4 = 256 // 4  # Un cuarto de la longitud de la ventana
W_LEN_3_4 = 3 * (256 // 4)  # Tres cuartos de la longitud de la ventana

def process_record(record_path, annotation_path):
    # Leer el registro desde el archivo
    record = wfdb.rdrecord(record_path)
    # Leer las anotaciones desde el archivo
    annotation = wfdb.rdann(annotation_path, 'atr')

    # Obtener la se침al y la frecuencia de muestreo
    signal = record.p_signal[:, 0]  # Solo el primer canal
    sampling_rate = record.fs

    # Procesar la se침al con NeuroKit para limpiarla
    signals, info = nk.ecg_process(signal, sampling_rate=sampling_rate)
    signal = signals["ECG_Clean"]  # Se침al limpia
    r_peaks_annot = info["ECG_R_Peaks"]  # Posiciones de los picos R

    # Segmentar latidos de la se침al
    segmented_signals, refined_r_peaks = segmentSignals(signal, r_peaks_annot)
    return segmented_signals

def process_person(person_folder, person_id):
    # Inicializar listas para almacenar segmentos y etiquetas
    all_segments = []
    all_labels = []
    segmentos = sorted(glob.glob(os.path.join(person_folder, '*.hea')))[:2]

    # Iterar sobre cada archivo en la carpeta de la persona
    for record_path in segmentos:
        base = record_path[:-4]  # Eliminar la extensi칩n del archivo
        annotation_path = base  # Ruta de las anotaciones

        # Procesar el archivo y segmentar los latidos
        segments = process_record(base, annotation_path)
        all_segments.extend(segments)  # Agregar segmentos
        all_labels.extend([person_id] * len(segments))  # Agregar etiquetas correspondientes

    # Convertir las listas en arrays de NumPy
    return np.array(all_segments), np.array(all_labels)

# Graficar las curvas de p칠rdida y precisi칩n
def plot_training_curves(history):
    # Obtener las m칠tricas del historial
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']
    train_acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    epochs_range = range(1, len(history.history['loss']) + 1)

    # Configurar el tama침o de la figura
    plt.figure(figsize=(12, 5))

    # Subplot 1: Curvas de p칠rdida
    plt.subplot(1, 2, 1)
    plt.plot(epochs_range, train_loss, label='Training Loss')
    plt.plot(epochs_range, val_loss, label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Loss Curve')
    plt.legend()

    # Subplot 2: Curvas de precisi칩n
    plt.subplot(1, 2, 2)
    plt.plot(epochs_range, train_acc, label='Training Accuracy')
    plt.plot(epochs_range, val_acc, label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Accuracy Curve')
    plt.legend()

    plt.show()

# 游늷 Verifica si hay GPU disponible
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Usando dispositivo:", device)

# 游늷 Cargar datos
base_folder = "BBDD/ecg-id-database-1.0.0"
X = []  # Lista para las se침ales
y = []  # Lista para las etiquetas

# Procesar los datos
for person_id, person_folder in enumerate(sorted(glob.glob(os.path.join(base_folder, 'Person_*')))):
    print(f"Procesando persona: {person_id}")
    segments, labels = process_person(person_folder, person_id)  # Funci칩n que procesa se침ales
    X.extend(segments)
    y.extend(labels)

# Convertir datos a tensores de PyTorch
X = torch.tensor(np.array(X), dtype=torch.float32).unsqueeze(1)  # A침adir canal
y = torch.tensor(np.array(y), dtype=torch.long)

# One-hot encoding
num_classes = 90
y = F.one_hot(y, num_classes=num_classes).float()

# 游늷 Dividir en 80% entrenamiento + validaci칩n y 20% prueba
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=torch.argmax(y, dim=1)
)

# Guardar conjunto de prueba para evaluaciones futuras
np.save("x_test.npy", X_test.numpy())
np.save("y_test.npy", y_test.numpy())

# 游늷 Stratified KFold
kf = KFold(n_splits=2, shuffle=True, random_state=42)
fold_accuracies = []
best_model = None
best_accuracy = 0.0  
epochs = 2


for fold, (train_index, val_index) in enumerate(kf.split(X_train, torch.argmax(y_train, dim=1))):
    print(f"Fold {fold}")

    # Dividir datos
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]


    # Crear DataLoaders
    train_dataset = TensorDataset(X_train_fold, y_train_fold)
    val_dataset = TensorDataset(X_val_fold, y_val_fold)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # Instanciar el modelo
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CNNModel(seq_len=W_LEN, n_classes=90).to(device)  # Mover modelo a GPU si est치 disponible
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    early_stopping_patience = 10  # N칰mero de 칠pocas sin mejora antes de detener el entrenamiento
    best_val_loss = float("inf")
    epochs_no_improve = 0


    # Entrenamiento
    for epoch in range(epochs):
        model.train()
        running_loss = 0.0
        train_loader_tqdm = tqdm(train_loader, desc=f"칄poca {epoch+1}/{epochs}")
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, torch.argmax(y_batch, dim=1))
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            # progreso
            train_loader_tqdm.set_postfix(loss=running_loss)

        # Validaci칩n
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                _, predicted = torch.max(outputs, 1)
                total += y_batch.size(0)
                correct += (predicted == torch.argmax(y_batch, dim=1)).sum().item()

        val_accuracy = correct / total
        fold_accuracies.append(val_accuracy)
        print(f"Fold {fold + 1} - Accuracy en validaci칩n: {val_accuracy:.4f}")

        # Guardar el mejor modelo
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            best_model = model
            torch.save(model.state_dict(), "Pytorch/model_Dummy.pth")  # Guardar mejor modelo

# 游늷 Matriz de Confusi칩n
y_labels = torch.argmax(y, dim=1).cpu().numpy()
conf_matrix = confusion_matrix(y_labels, y_labels)


plt.figure(figsize=(15, 15))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Clase")
plt.ylabel("Clase")
plt.title("Matriz de Confusi칩n de Frecuencias por Clase (Desbalanceo de Datos)")
plt.savefig("Pytorch/img/train/ConfMatrix.png")
plt.show()

# 游늷 Curva ROC para la clase 2 y 15 clases aleatorias
random_classes = np.random.choice(range(90), 15, replace=False).tolist()
classes_to_plot = [2] + random_classes  

plt.figure(figsize=(10, 7))

for class_id in classes_to_plot:
    fpr, tpr, _ = roc_curve(
        y_val_fold[:, class_id].cpu().numpy(),  # Cambia y_test por y_val_fold
        model(X_val_fold.to(device))[:, class_id].cpu().detach().numpy()
    )
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"Clase {class_id} (AUC = {roc_auc:.2f})")


plt.plot([0, 1], [0, 1], 'k--', label="Chance Level (AUC = 0.50)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Curva ROC (One-vs-Rest) para 16 clases")
plt.legend()
plt.savefig("Pytorch/img/train/roc_curve.png")  # Guarda la curva ROC como imagen
plt.show()
